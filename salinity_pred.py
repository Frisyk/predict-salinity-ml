# -*- coding: utf-8 -*-
"""notebook2d192108b1

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/#fileId=https%3A//storage.googleapis.com/kaggle-colab-exported-notebooks/notebook2d192108b1-9c24eb50-a533-4066-8309-d305d0546407.ipynb%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dgcp-kaggle-com%2540kaggle-161607.iam.gserviceaccount.com/20250226/auto/storage/goog4_request%26X-Goog-Date%3D20250226T161434Z%26X-Goog-Expires%3D259200%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3D29f91d6f4d4bb457f24619089647cec745dd37ebe507333718daf862232c46d94e2c185ecf93c40b2d466b78684e008b0390f200262365d9e1d15d874ce4e94190e6c2742eb72415d5504c1115806d2b2420c2887be24499dba91547022f9cdc472e242abfc77cb0a0735dd141e5514aef76006a1b27c054cad156ced69ce332a8fb4c98ef48353b1e02f23468a4080277857acf769116416e4733a6037ca4d8dac77a51af6e84d56aba04c674cdb1a55074b63599c2972edb3c1f4a9eec281240a56557ef0328056f7fd81a6a0407624283a76bec784aee6c2d6fc67a32b1e095f7ab757a040cf3043bce3a08e6200f4801b75482eb60fb3669a648dd81558b
"""

# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES,
# THEN FEEL FREE TO DELETE THIS CELL.
# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON
# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR
# NOTEBOOK.
import kagglehub
lainguyn123_student_performance_factors_path = kagglehub.dataset_download('lainguyn123/student-performance-factors')
patricklford_underway_pco_and_ocean_data_rv_j_clark_ross_path = kagglehub.dataset_download('patricklford/underway-pco-and-ocean-data-rv-j-clark-ross')
adilshamim8_education_and_career_success_path = kagglehub.dataset_download('adilshamim8/education-and-career-success')
zabihullah18_student_performance_path = kagglehub.notebook_output_download('zabihullah18/student-performance')

print('Data source import complete.')

"""# Data Loading"""

# Commented out IPython magic to ensure Python compatibility.
import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
# %matplotlib inline
import seaborn as sns
import os

folder_path = "/kaggle/input/underway-pco-and-ocean-data-rv-j-clark-ross"

csv_files = [file for file in os.listdir(folder_path) if file.endswith('.csv')]

df_list = [pd.read_csv(os.path.join(folder_path, file)) for file in csv_files]
df = pd.concat(df_list, ignore_index=True)

df.head()

"""# Exploratory Data Analysis (EDA)

## Variable Description
"""

df.info()

# mengubah nama fitur agar memudahkan pemahaman

rename_columns = {
    "JD_GMT": "waktu_julian",
    "LAT_dec_degree": "latitude",
    "LONG_dec_degree": "longitude",
    "xCO2_equ[umol/mol]": "kadar_CO2_air",
    "Patm [hPa]": "tekanan_udara",
    "Tequ [deg.C]": "suhu_air_sensor",
    "SST [deg.C]": "suhu_permukaan_laut",
    "Sal": "salinitas",
    "pCO2_sw[uatm]": "tekanan_CO2_air",
    "pCO2_atm[uatm]": "tekanan_CO2_udara",
    "fCO2_sw[uatm]": "fraksi_CO2_air",
    "fCO2_atm[uatm]": "fraksi_CO2_udara",
    "xCO2atm_dry[umol/mol]": "kadar_CO2_kering",
    "Pequ [hPa]": "tekanan_air",
    "DATE_UTC__ddmmyyyy": "tanggal",
    "TIME_UTC_hh:mm:ss": "waktu"
}

df.rename(columns=rename_columns, inplace=True)

df.head()

df.describe()

df.isnull().sum()

df.shape

"""## Handling Missing Value and Outliers"""

from sklearn.impute import KNNImputer

imputer = KNNImputer(n_neighbors=5)
df[['tekanan_CO2_udara', 'fraksi_CO2_udara', 'kadar_CO2_kering']] = imputer.fit_transform(df[['tekanan_CO2_udara', 'fraksi_CO2_udara', 'kadar_CO2_kering']])

df.isnull().sum()

num_features = df.select_dtypes(include=['number']).columns

# Tentukan jumlah subplot
num_plots = len(num_features)
rows = (num_plots // 5) + (num_plots % 5 > 0)  # Atur jumlah baris secara otomatis
fig, axes = plt.subplots(nrows=rows, ncols=5, figsize=(15, 5 * rows))

# Flatten axes untuk iterasi yang lebih mudah
axes = axes.flatten()

# Plot masing-masing boxplot di subplot
for i, feature in enumerate(num_features):
    sns.boxplot(x=df[feature], ax=axes[i])
    axes[i].set_title(feature)

# Hapus subplot kosong jika ada
for j in range(i + 1, len(axes)):
    fig.delaxes(axes[j])

plt.tight_layout()
plt.show()

numerical_outlier = [
    "tekanan_udara", "suhu_air_sensor", "suhu_permukaan_laut", "salinitas",
    "tekanan_CO2_udara", "fraksi_CO2_udara",
    "kadar_CO2_kering", "tekanan_air"
]

Q1 = df[numerical_outlier].quantile(0.25)
Q3 = df[numerical_outlier].quantile(0.75)
IQR = Q3 - Q1

# Menentukan batas bawah dan atas
lower_bound = Q1 - 1.5 * IQR
upper_bound = Q3 + 1.5 * IQR

# Menghapus outlier
df = df[~((df[numerical_outlier] < lower_bound) | (df[numerical_outlier] > upper_bound)).any(axis=1)]
df.shape

num_features = df.select_dtypes(include=['number']).columns

# Tentukan jumlah subplot
num_plots = len(num_features)
rows = (num_plots // 5) + (num_plots % 5 > 0)  # Atur jumlah baris secara otomatis
fig, axes = plt.subplots(nrows=rows, ncols=5, figsize=(15, 5 * rows))

# Flatten axes untuk iterasi yang lebih mudah
axes = axes.flatten()

# Plot masing-masing boxplot di subplot
for i, feature in enumerate(num_features):
    sns.boxplot(x=df[feature], ax=axes[i])
    axes[i].set_title(feature)

# Hapus subplot kosong jika ada
for j in range(i + 1, len(axes)):
    fig.delaxes(axes[j])

plt.tight_layout()
plt.show()

"""## Univariate Analysis"""

numerical_features = [
    "waktu_julian", "latitude", "longitude", "kadar_CO2_air",
    "tekanan_udara", "suhu_air_sensor", "suhu_permukaan_laut", "salinitas",
    "tekanan_CO2_air", "tekanan_CO2_udara", "fraksi_CO2_air", "fraksi_CO2_udara",
    "kadar_CO2_kering", "tekanan_air"
]

df.hist(bins=50, figsize=(20,15))
plt.show()

"""## Multivariate Analysis"""

cat_features = df.select_dtypes(include='number').columns.to_list()

for col in cat_features:
  sns.catplot(x=col, y="tekanan_CO2_air", kind="bar", dodge=False, height = 4, aspect = 3,  data=df, palette="Set3")
  plt.title("Rata-rata 'tekanan_CO2_air' Relatif terhadap - {}".format(col))

df.replace([np.inf, -np.inf], np.nan, inplace=True)
sns.pairplot(df, diag_kind = 'kde')

plt.figure(figsize=(10, 8))
correlation_matrix = df[numerical_features].corr().round(2)

# Untuk menge-print nilai di dalam kotak, gunakan parameter anot=True
sns.heatmap(data=correlation_matrix, annot=True, cmap='coolwarm', linewidths=0.5, )
plt.title("Correlation Matrix untuk Fitur Numerik ", size=20)

features = ["tekanan_udara", "suhu_air_sensor", "suhu_permukaan_laut", "tekanan_air", "kadar_CO2_air"]

# Set ukuran figure
plt.figure(figsize=(12, 8))

for i, feature in enumerate(features, 1):
    plt.subplot(2, 3, i)
    sns.scatterplot(x=df[feature], y=df["salinitas"], alpha=0.7)
    sns.regplot(x=df[feature], y=df["salinitas"], scatter=False, color='red')
    plt.xlabel(feature)
    plt.ylabel("Salinitas")
    plt.title(f"Salinitas vs {feature}")

plt.tight_layout()
plt.show()

"""# Data Preparation"""

df.drop(columns=["tanggal", "waktu", "waktu_julian", "latitude", "longitude", "tekanan_CO2_udara","fraksi_CO2_air", "fraksi_CO2_udara", "tekanan_CO2_air", "kadar_CO2_kering" ], inplace=True)
df.head()

"""## Dimensionality Reduction with PCA"""

from sklearn.decomposition import PCA

features_suhu = ["suhu_air_sensor", "suhu_permukaan_laut"]
features_tekanan = ["tekanan_air", "tekanan_udara"]

pca_suhu = PCA(n_components=1, random_state=42)
df["komponen_suhu"] = pca_suhu.fit_transform(df[features_suhu]).flatten()

pca_tekanan = PCA(n_components=1, random_state=42)
df["komponen_tekanan"] = pca_tekanan.fit_transform(df[features_tekanan]).flatten()

df.drop(features_suhu + features_tekanan, axis=1, inplace=True)

print(f"Variansi yang dijelaskan oleh komponen suhu: {pca_suhu.explained_variance_ratio_[0]:.2f}")
print(f"Variansi yang dijelaskan oleh komponen tekanan: {pca_tekanan.explained_variance_ratio_[0]:.2f}")

"""## Train-Test-Split"""

from sklearn.model_selection import train_test_split

X = df.drop(columns=["salinitas"])
y = df["salinitas"]

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=42)

print(f"Jumlah data latih: {X_train.shape[0]}")
print(f"Jumlah data uji: {X_test.shape[0]}")

"""## Standardization"""

from sklearn.preprocessing import StandardScaler

# Daftar fitur numerik yang tersisa setelah PCA
numerical_features = ["komponen_suhu", "komponen_tekanan", "kadar_CO2_air"]

scaler = StandardScaler()

scaler.fit(X_train[numerical_features])
X_train[numerical_features] = scaler.transform(X_train.loc[:, numerical_features])

X_test[numerical_features] = scaler.transform(X_test.loc[:, numerical_features])

X_train[numerical_features].head()

X_train[numerical_features].describe().round(4)

df.head()

"""# Modeling"""

from sklearn.neighbors import KNeighborsRegressor
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
from xgboost import XGBRegressor
from sklearn.model_selection import GridSearchCV
from sklearn.model_selection import ShuffleSplit

"""## Grid Search"""

from tabulate import tabulate

def grid_search_model(X, y):
    algos = {
        'KNN': {
            'model': KNeighborsRegressor(),
            'params': {
                'n_neighbors': [5, 7, 9, 11, 13, 15],
                'weights': ['uniform', 'distance']
            }
        },
        'Random Forest': {
            'model': RandomForestRegressor(),
            'params': {
                'n_estimators': [50, 100, 150],
                'max_depth': [8, 16, 32],
                'random_state': [42]
            }
        },
        'Gradient Boosting': {
            'model': GradientBoostingRegressor(),
            'params': {
                'n_estimators': [50, 100, 150],
                'learning_rate': [0.01, 0.05, 0.1],
                'max_depth': [3, 5, 7]
            }
        },
        'XGBoost': {
            'model': XGBRegressor(),
            'params': {
                'n_estimators': [50, 100, 150],
                'learning_rate': [0.01, 0.05, 0.1],
                'max_depth': [3, 5, 7]
            }
        }
    }

    scores = []
    best_models = {}
    cv = ShuffleSplit(n_splits=5, test_size=0.2, random_state=123)

    for algo_name, config in algos.items():
        print(f"Training {algo_name}...")
        gs = GridSearchCV(config['model'], config['params'], cv=cv, scoring='r2', n_jobs=-1)
        gs.fit(X, y)
        best_models[algo_name] = gs.best_estimator_
        scores.append({
            'Model': algo_name,
            'Best Score': round(gs.best_score_, 4),
            'Best Params': gs.best_params_
        })

    df_results = pd.DataFrame(scores)

    print(tabulate(df_results, headers="keys", tablefmt="grid"))

    return df_results, best_models

results, best_models = grid_search_model(X_train, y_train)

"""## Best Algorithm"""

# KNN
best_knn = best_models['KNN']

# Random Forest
best_rf = best_models['Random Forest']

# Gradient Boosting
best_gb = best_models['Gradient Boosting']

# XGBoost
best_xgb = best_models['XGBoost']

"""# Evaluation"""

from sklearn.metrics import mean_squared_error, r2_score
from tabulate import tabulate

model_dict = {
    'KNN': best_knn,
    'Random Forest': best_rf,
    'Gradient Boosting': best_gb,
    'XGBoost': best_xgb
}

results = []
mse = pd.DataFrame(columns=['Train MSE', 'Test MSE'], index=model_dict.keys())

for name, model in model_dict.items():
    y_pred = model.predict(X_test)

    results.append([name, mean_squared_error(y_test, y_pred), r2_score(y_test, y_pred)])

    # Hitung MSE untuk train & test
    mse.loc[name] = [mean_squared_error(y_train, model.predict(X_train)),
                     mean_squared_error(y_test, y_pred)]

df_results = pd.DataFrame(results, columns=['Model', 'MSE', 'RÂ²'])

print("\n### Model Performance ###")
print(tabulate(df_results, headers='keys', tablefmt='pretty', floatfmt=".4f"))

print("\n### Mean Squared Error (MSE) Comparison ###")
print(tabulate(mse, headers='keys', tablefmt='pretty', floatfmt=".4f"))

fig, ax = plt.subplots()
mse.sort_values(by='Test MSE', ascending=False).plot(kind='barh', ax=ax, zorder=3, color=['blue', 'orange', 'green', 'red'])
ax.set_title("Perbandingan MSE Model pada Data Test")
ax.set_xlabel("MSE")
ax.set_ylabel("Model")
ax.grid(zorder=0)
plt.show()

prediksi = X_test.iloc[5:10].copy()
pred_dict = {'y_true':y_test[5:10]}
for name, model in model_dict.items():
    pred_dict['prediksi_'+name] = model.predict(prediksi).round(1)

pd.DataFrame(pred_dict)